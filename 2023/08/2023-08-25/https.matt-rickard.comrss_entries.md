# Source:Matt Rickard, URL:https://matt-rickard.com/rss, language:en-US

## The Free Lunch of Model Distillation
 - [https://matt-rickard.com/the-free-lunch-of-model-distillation](https://matt-rickard.com/the-free-lunch-of-model-distillation)
 - RSS feed: https://matt-rickard.com/rss
 - date published: 2023-08-25T13:32:26.055729+00:00

Model distillation uses one model to generate training data for a second model. It’s been shown that this synthetic data can significantly improve models and distill knowledge (I prefer to think of it in finance terms as model arbitrage).

Meta released its Code Llama models — LLMs built for code generation based on the Llama family. One model was missing from the downloadable Code Llama model weights despite being described in the paper as an “Unnatural Llama.” This model was trained on synthet

