[{"source": "https://matt-rickard.com/rss", "title": "The Free Lunch of Model Distillation", "description": "Model distillation uses one model to generate training data for a second model. It\u2019s been shown that this synthetic data can significantly improve models and distill knowledge (I prefer to think of it in finance terms as model arbitrage).\n\nMeta released its Code Llama models \u2014 LLMs built for code generation based on the Llama family. One model was missing from the downloadable Code Llama model weights despite being described in the paper as an \u201cUnnatural Llama.\u201d This model was trained on synthet", "link": "https://matt-rickard.com/the-free-lunch-of-model-distillation", "date_published": "2023-08-25T13:32:26.055729+00:00", "persistent": false, "dead": false, "artist": "Matt Rickard", "album": "Matt Rickard", "user": null, "language": "en-US", "thumbnail": null}]